{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/forestlinlinlinlinlin/Bollinger-Bands-exercise/blob/main/T2_rdd_operator_lab_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial 2: Programming with RDDs \n",
        "In this tutorial, you will learn how to create a RDD. RDD represents **Resilient Distributed Dataset**. An RDD in Spark is simply an immutable distributed collection of objects sets. Each RDD is split into multiple partitions (similar pattern with smaller sets), which may be computed on different nodes of the cluster."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "ppa2H38p5Vyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "JIt9Tb6u5fnV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8140df15-a623-4056-c939-ef50956c91fe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 35 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.2\n",
            "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 54.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805911 sha256=4851ed2d0855c7108ccca26e359d47490c5a74d278b4684b0a77c031809d5532\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/de/d2/9be5d59d7331c6c2a7c1b6d1a4f463ce107332b1ecd4e80718\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.2 pyspark-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create SparkContext and SparkSession\n",
        "Remeber to create a session, which is an entry point to Spark"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "eN7ZeMX45Vyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create entry points to spark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "ss  = SparkSession.builder \\\n",
        "                            .master(\"local[1]\")\\\n",
        "                            .appName(\"SparkByExamples.com\")\\\n",
        "                            .getOrCreate()\n",
        "spark = ss.sparkContext"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639389904453
        },
        "id": "maETJ9uC5Vyo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Create RDD\n",
        "The class `pyspark.SparkContext` creates a client which connects to a Spark cluster. This client can be used to create an RDD object. There are two methods from this class for directly creating RDD objects:\n",
        "* `parallelize()`\n",
        "* `textFile()`"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "nMRvVnu75Vyq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1.1 `Parallelize()`\n",
        "\n",
        "`parallelize()` distribute a local **python collection** to form an RDD. Common built-in python collections include `dist`, `list`, `tuple` or `set`.\n",
        "\n",
        "Examples:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "KxjxFAvw5Vyr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important**: Use `collect()` to get back the value in RDD"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "d45K5f7h5Vyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from a list\n",
        "rdd_data = spark.parallelize([1,2,3])\n",
        "rdd_data"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639389910039
        },
        "id": "WbQvhn6W5Vys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_data.collect()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639389915106
        },
        "id": "osLMYja65Vyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from a tuple\n",
        "rdd_data = spark.parallelize(('cat', 'dog', 'fish'))\n",
        "rdd_data.collect()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639389920342
        },
        "id": "LMQzKXur5Vyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from a list of tuple\n",
        "list_t = [('cat', 'dog', 'fish'), ('orange', 'apple')]\n",
        "rdd_data = spark.parallelize(list_t)\n",
        "rdd_data.collect()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639389925603
        },
        "id": "QbLgXFf95Vyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from a set\n",
        "s = {'cat', 'dog', 'fish', 'cat', 'dog', 'dog'}\n",
        "rdd_data = spark.parallelize(s)\n",
        "rdd_data.collect()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639389930979
        },
        "id": "g4cjUwAm5Vyv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-4d9EqBQ8xdd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1.2 `textfile()`\n",
        "\n",
        "`textfile()` convert a txt file to RDD object.\n",
        "\n",
        "Examples:"
      ],
      "metadata": {
        "id": "7OsDNCjc8LQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# these line are used to load the Google drive into the notebook\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "data_path = \"/content/drive/MyDrive/Colab Notebooks/\"  # this is your drive\n"
      ],
      "metadata": {
        "id": "YUNGRoMe88wC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f55a75e-2ba3-4b1f-abb8-e05331abf5c6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "textFile = spark.textFile(data_path + \"wordCount.txt\") # use this if you are using Colab\n",
        "# textFile = spark.textFile(\"wordCount.txt\")\n",
        "textFile.collect()"
      ],
      "metadata": {
        "id": "zCbRcN9k8T4y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e2bc3d6-4eab-48cf-f1c9-4815412b7593"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I am Billy studying in Computer', 'Ada is studying in Computer']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 RDD transformation (Single RDD)\n",
        "RDD transformation is a list of **operations** that apply on the source RDD and construct a new RDD \n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "Cgl91eQ25Vyw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2.1 `Map()`\n",
        "\n",
        "`map()` returns a new RDD formed by passing each element of the source RDD throguh a function \n",
        "\n",
        "Examples:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "sBamBI6t5Vyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function\n",
        "def addOne(x):\n",
        "    return x+1"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639389933585
        },
        "id": "QaSvFVN35Vyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_data = spark.parallelize([1,2,3])\n",
        "results = rdd_data.map(addOne)\n",
        "results.collect()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639389938930
        },
        "id": "MKwcEDjx5Vyy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.1.1 : map() example 1\n",
        "\n",
        "Use ``map()`` to perform **map** transformation, add one to each element in RDD, and use ``collect()`` to get the results"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "hLOWe0FM5Vyy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of function, you can use a **closure** instead"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "oi5mx5mz5Vyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results1 = rdd_data.map(lambda x: x+1)\n",
        "results1.collect()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639389944246
        },
        "id": "WeRu_JCo5Vyz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.1.2 : map() example 2\n",
        "\n",
        "``map()`` can also apply on **String** datatype"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "khN2qxcz5Vy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_t1 = ['cat', 'dog', 'fish', 'orange', 'apple']\n",
        "rdd_data1 = spark.parallelize(list_t1)\n",
        "\n",
        "results2 = rdd_data1.map(lambda x: 'object: '+ x)\n",
        "results2.collect()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639389949532
        },
        "id": "tnyfIzxZ5Vy0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.1.3: map() example 3\n",
        "\n",
        "``map()`` can also apply to create **(String, Int)** Pair"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "sQu2nOvS5Vy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_t1 = ['cat', 'cat', 'dog', 'fish', 'orange', 'apple']\n",
        "rdd_data1 = spark.parallelize(list_t1)\n",
        "\n",
        "results2 = rdd_data1.map(lambda x: (x, 1))\n",
        "results2.collect()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639389954901
        },
        "id": "w03Z5GaH5Vy0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2.2 ``Filter()``\n",
        "``Filter()`` returns a new dataset formed by selecting those elements of the source on which function return **true**. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "mzaGpkM45Vy1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.2.1 : filter() example 1"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "THenZpid5Vy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_data2 = spark.parallelize([3,1,2,5,5])\n",
        "result3 = rdd_data2.filter(lambda x: x > 2)\n",
        "result3.collect()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639389960219
        },
        "id": "wYWQ5it65Vy1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.2.2 : filter() example 2\n",
        "\n",
        "Let's try filter with **multiple** condition "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "a87SxVr75Vy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_data2 = spark.parallelize([3,1,2,6,5])\n",
        "result3 = rdd_data2.filter(lambda x: (x > 2) & (x % 2 ==0)) # larger than 2 and divisible by 2\n",
        "result3.collect()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639389965433
        },
        "id": "ssN1POTA5Vy1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2.3 ``Distinct()``\n",
        "``Distinct()`` is used to filter out repetitive items"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "dV74cghO5Vy2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.3.1: distinct() example 1"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "Y2LvgXSc5Vy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_data2 = spark.parallelize([3,1,2,5,5,5])\n",
        "result4 = rdd_data2.distinct()\n",
        "result4.collect()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639389970639
        },
        "id": "H_oLKY_R5Vy2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.3.2 : distinct() example 2\n",
        "Similar to example 2.4.2, just this time we use **string** instead of **integer** "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "WBRT1Vgq5Vy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_data2 = spark.parallelize(['cat', 'cat', 'dog', 'fish', 'orange', 'apple'])\n",
        "result5 = rdd_data2.distinct()\n",
        "result5.collect()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639389975877
        },
        "id": "eNFDmkuI5Vy2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2.4 ``RandomSplit()``\n",
        "``randomSplit()`` can be used to divided an RDD into multiple sub segments, it can randomly splites this RDD with \n",
        "the provided weights. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "9t9qW5wL5Vy3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.4.1: randomSplit() example 1\n",
        "Split the RDD into 60/40"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "fXKNAoGB5Vy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_data3 = spark.parallelize(range(10), 1) # create a list of 1 to 10\n",
        "print('full data: %s'% rdd_data3.collect()) \n",
        "\n",
        "res = rdd_data3.randomSplit([0.6, 0.4])  # here, you will have two segment return\n",
        "\n",
        "## obtain the first segment\n",
        "print('first half: %s'% res[0].collect())\n",
        "\n",
        "## obtain the second segment\n",
        "print('second half: %s'% res[1].collect())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639389980935
        },
        "id": "n8pExifw5Vy3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.4.2 (Important): randomSplit() example 2 (RandomSplit in a non-random way)\n",
        "Often, you need to split the dataset in a way that you can **reproduce** the result. \n",
        "Then you will pre-define a seed so that each time the splitting pattern is identical.\n",
        "You can re-run example 2.5.2 to see if spliting results in each trial is the same."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "CzpWNud05Vy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_data3 = spark.parallelize(range(10), 1)\n",
        "res = rdd_data3.randomSplit([0.6, 0.4], seed=1234)  # here, give a fix seed to ensure every time splitting is same \n",
        "\n",
        "## obtain the first segment\n",
        "print('first half: %s'% res[0].collect())\n",
        "\n",
        "## obtain the second segment\n",
        "print('second half: %s'% res[1].collect())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639389988812
        },
        "id": "BEOUjq-b5Vy3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2.5 ``GroupBy()``\n",
        "Return an RDD of grouped items based on a pre-defined **condition**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "Ha1n0QPP5Vy3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.5.1 : groupBy() example 1\n",
        "Grouped by even number"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "0Cz-ab9y5Vy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_data4 = spark.parallelize([1, 1, 2, 3, 5, 8])\n",
        "result = rdd_data4.groupBy(lambda x: x % 2)  # groupby number divisible by 2\n",
        "\n",
        "# groupBy will return a key-value pair\n",
        "res = result.collect()\n",
        "\n",
        "# to view it, we need to convert the value to list\n",
        "print([(x, list(y)) for (x, y) in res])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639389993937
        },
        "id": "21HxYenX5Vy4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2.6 ``flatMap()``\n",
        "Sometimes we want to produce multiple output elements for each input elements. The operation to do this is called\n",
        "``flatMap()``."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "u942vcfv5Vy4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.6.1: flatMap() example 1\n",
        "For example, we can apply a ``split()`` function on each phrase in the RDD to split them by space"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "bFb10IPN5Vy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_data7 = spark.parallelize(['hello world', 'hi', 'dog', 'fish', 'orange', 'apple'])\n",
        "result7 = rdd_data7.flatMap(lambda x: x.split(' ')) # split each phrase by space\n",
        "result7.collect()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello', 'world', 'hi', 'dog', 'fish', 'orange', 'apple']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639389999165
        },
        "id": "75jydbfQ5Vy5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1328d28-6aa9-4097-fb10-43f077b50d9d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.6.2: ``flatMap()`` vs ``map()``\n",
        "``map()`` will not **flatten** the results. Hence, 3 input will have 3 output. \n",
        "In contrast, ``flatMap()`` flatten the output, so 3 input will have 1 output."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "l_YHqaEP5Vy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_data7 = spark.parallelize(['hello world', 'hi', 'dog', 'fish', 'orange', 'apple'])\n",
        "result7 = rdd_data7.flatMap(lambda x: x.split(' ')) # split each phrase by space\n",
        "print(\"Flatmap(): %s\" % result7.collect())\n",
        "\n",
        "result8 = rdd_data7.map(lambda x: x.split(' '))\n",
        "print(\"map(): %s\" % result8.collect())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flatmap(): ['hello', 'world', 'hi', 'dog', 'fish', 'orange', 'apple']\n",
            "map(): [['hello', 'world'], ['hi'], ['dog'], ['fish'], ['orange'], ['apple']]\n"
          ]
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639390004403
        },
        "id": "4UHbPCFE5Vy5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24a8652b-db8d-4394-f869-81d19318dd58"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 RDD transformation (Multiple RDD)\n",
        "Here, we introduce transformation operators that apply on multiple RDD (e.g., union, intersection)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "a46rchbV5Vy6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3.1 ``Union()``\n",
        "Return the **combination** of all input RDD"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "AYtMauXT5Vy6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3.1.1 ``union()`` example 1"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "ACbY7BtG5Vy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rddData1 = spark.parallelize([1, 1, 2, 3])\n",
        "rddData2 = spark.parallelize([4, 5, 6, 7])\n",
        "rddData3 = spark.parallelize([21, 22, 23, 25])\n",
        "rddData1.union(rddData2).union(rddData3).collect()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639390009761
        },
        "id": "BmE3O70c5Vy6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3.2 ``Intersection()``\n",
        "Return the **overlap** of all input RDD. For example, {1,2,3} interaction {3,4,5} = {3}"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "FILi4JyF5Vy6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3.2.1 intersection example 1"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "z9_XezJV5Vy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rddData1 = spark.parallelize([1, 1, 2, 3])\n",
        "rddData2 = spark.parallelize([3, 4, 5, 7])\n",
        "rddData1.intersection(rddData2).collect()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639390014886
        },
        "id": "CJ4-RATi5Vy6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3.3 ``Subtract()``\n",
        "Return each value in self that is not contained in other. For example {1,2,3} subtract {3,4,5} = {1,2}"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "YLMzsu3g5Vy7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3.3.1 subtract example 1"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "-HjjJCOM5Vy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rddData1 = spark.parallelize([1, 1, 2, 3])\n",
        "rddData2 = spark.parallelize([3, 4, 5, 7])\n",
        "sorted(rddData1.subtract(rddData2).collect())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639390019962
        },
        "id": "grxRv0y85Vy7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3.4 ``Cartesian()`` \n",
        "Return the Cartesian product of this RDD and another one. \n",
        "For example, {1,2,3} cartesian {3,4,5} = {{1,3}, {1,4}...{3,5}}"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "VJziduk15Vy7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3.4.1 cartesian() example 1"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "gk-evxR55Vy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rddData1 = spark.parallelize([1, 1, 2, 3])\n",
        "rddData2 = spark.parallelize([3, 4, 5, 7])\n",
        "rddData1.cartesian(rddData2).collect()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639390025211
        },
        "id": "9SKf8lJs5Vy7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 RDD Actions\n",
        "Actions operators will create non-rdd values. For example ``collect()`` is an action opeator."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "1lUksZ6N5Vy8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4.1 ``first()`` \n",
        "Return the first element in the RDD"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "WHoy6vFm5Vy8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4.1.1: first() example 1"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "dx5gwEqV5Vy8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rddData1 = spark.parallelize([21, 1, 2, 3])\n",
        "rddData1.first()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639390030450
        },
        "id": "Qk99xY6S5Vy8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4.2 ``take()``\n",
        "Get the first N elements in the RDD "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "KsUSheK-5Vy8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4.2.1: take() example 1"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "E1vzCc_j5Vy8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rddData1 = spark.parallelize([21, 1, 2, 3])\n",
        "rddData1.take(2)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639390035658
        },
        "id": "0HYCByCR5Vy8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4.3 ``takeOrder()``\n",
        "Get the N elements from an RDD ordered in ascending order or as specified by the optional key function"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "tYbo44h85Vy9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4.3.1: takeOrder() example 1\n",
        "You can take the first N item in ascending order"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "uLAlA2lB5Vy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639390040782
        },
        "id": "9gZ50nyQ5Vy9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4.3.2: takeOrder() example 2\n",
        "You can take the first N item in **decending** order"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "i8GbhbG25Vy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6, lambda x: -1*x)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639390046227
        },
        "id": "-7RGW3Gi5Vy9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 Basic Statistics in RDD\n",
        "Here, we will learn few RDD operators for getting the statistics in RDD"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "OWs8D_fe5Vy-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5.1 ``min()``\n",
        "Get the minimum value in the RDD"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "EiNnEzIH5Vy-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5.1.1: min() example 1"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "yth_HQ1i5Vy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).min()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639390051360
        },
        "id": "tv5LNpYw5Vy-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5.2 ``max()``\n",
        "Get the maximum value in the RDD"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "ndle9h2X5Vy-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5.2.1: max() example 1"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "rFL3IpzH5Vy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).max()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639390056414
        },
        "id": "EvpLXPTP5Vy-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5.3 ``stdev()``\n",
        "Get the standard deviation in the RDD"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "Y4bgotPD5Vy-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5.3.1: stdev() example 1"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "ZuuiDkKu5Vy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).stdev()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639390061695
        },
        "id": "jybrfrtY5Vy_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5.4 ``count()``\n",
        "Find the number of elements in this RDD."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "0jI3dwCn5Vy_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5.4.1: count() example 1"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "pxUal6xl5Vy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.parallelize([2, 3, 4]).count()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639390066805
        },
        "id": "ReZMIG465Vy_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5.5 ``sum()``\n",
        "Find the sum of elements in this RDD."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "Zo8yzmOi5Vy_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5.5.1: sum() example 1"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "BSwLEQLK5Vy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.parallelize([1, 2, 3, 4]).sum()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639390071973
        },
        "id": "cQHe2hLg5Vy_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5.6 ``mean()``\n",
        "Find the average of elements in this RDD."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "KSuJqqUs5Vy_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5.6.1: mean() example 1"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "8_ndNXpw5VzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.parallelize([1, 2, 3, 4]).mean()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639390077241
        },
        "id": "mZak3CrA5VzA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.6 Single *key-value* pair transformations\n",
        "Key-value pair RDD is a specific type of RDD, in which the data will be stored in *key-value* format.\n",
        "The *key* serves as a unique ID for retrieving the values in the RDD. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "aQXRq3e85VzA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6.1 Get elements"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "klF2tQIM5VzA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.6.1.1: get all key"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "LPle_gCj5VzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kvRDD = spark.parallelize([(3,4), (3,6), (5,6), (1,2)])\n",
        "kvRDD.keys().collect()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 3, 5, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639390082369
        },
        "id": "AdH5TgQ35VzA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78267af9-ee4f-414d-cc21-a38c6899d17f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.6.1.2: get all values"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "aEtmY3e95VzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kvRDD = spark.parallelize([(3,4), (3,6), (5,6), (1,2)])\n",
        "kvRDD.values().collect()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4, 6, 6, 2]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639390087549
        },
        "id": "DXL43wD75VzA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b8b6fd4-934c-4e1f-a89e-c5c480feb950"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6.2 Filter elements (``filter()``)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "383l-Z1m5VzB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.6.2.1: filter by keys\n",
        "filter all key value pair with key less than 5"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "ZO1XiBNT5VzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kvRDD = spark.parallelize([(3,4), (3,6), (5,6), (1,2)])\n",
        "kvRDD.filter(lambda kv: kv[0] < 5).collect()  #kv[0] = key"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639390092807
        },
        "id": "R8s9N0f_5VzB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.6.2.2: filter by values\n",
        "filter all key value pair with value less than 5"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "vgwfjIIV5VzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kvRDD = spark.parallelize([(3,4), (3,6), (5,6), (1,2)])\n",
        "kvRDD.filter(lambda kv: kv[1] < 5).collect()  #kv[1] = value"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639390098079
        },
        "id": "VR0jYXxa5VzB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6.3 Sort elements (``sortBy()``)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "-HuQ6Wn55VzB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.6.3.1: Sort by keys"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "2jZwKqYF5VzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kvRDD = spark.parallelize([(3,4), (3,1), (5,6), (1,2)])\n",
        "kvRDD.sortBy(lambda kv: kv[0], ascending=True).collect()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639390103150
        },
        "id": "zjECNueq5VzC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.6.3.1: Sort by values"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "Gc6b6cHa5VzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kvRDD = spark.parallelize([(3,4), (3,1), (5,6), (1,2)])\n",
        "kvRDD.sortBy(lambda kv: kv[1], ascending=True).collect()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639390108380
        },
        "id": "lcJ_txBJ5VzC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6.4 Map elements (``map()``)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "TE9NZ8KP5VzC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.6.4.1: apply map on each value\n",
        "Key-value map is similar to our previous case used in example 2.2.1. \n",
        "Just this time the ``map()`` is apply on key-value only.\n",
        "Here, we try replacing each key with its value"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "WvERucjk5VzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kvRDD = spark.parallelize([(3,4), (3,1), (5,6), (1,2)])\n",
        "kvRDD.map(lambda kv: (kv[1], kv[1])).collect()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639390113587
        },
        "id": "C_YhqIOd5VzC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.6.4.2: map(key value swap)\n",
        "This time, we swap the key and value"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "JNiMMCAN5VzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kvRDD = spark.parallelize([(3,4), (3,1), (5,6), (1,2)])\n",
        "kvRDD.map(lambda kv: (kv[1], kv[0])).collect()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639390118850
        },
        "id": "2RufEd_d5VzD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.6.4.3 ``mapValue()``\n",
        "We can use ``mapValues()`` to apply function on only value"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "PM4iiq8I5VzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kvRDD = spark.parallelize([(3,4), (3,1), (5,6), (1,2)])\n",
        "kvRDD.mapValues(lambda kv: kv**2).collect()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639390124039
        },
        "id": "HPyN8_nd5VzD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6.5 reduceByKey()\n",
        "``reduceByKey()`` means grouping the keys together, and apply a function on its values.\n",
        "For example, we can group all data with key=3, then add their values"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "TRm1KYQ85VzD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.6.5.1 group and add values"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "uEjavedV5VzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import add\n",
        "kvRDD = spark.parallelize([(3,4), (3,1), (5,6), (1,2)])\n",
        "kvRDD.reduceByKey(add).collect()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(3, 5), (5, 6), (1, 2)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639390129175
        },
        "id": "VnVDySlB5VzD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9606135-6af2-49bd-95a9-ab2e21ba0c1b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.6.5.2 group and add values (another way)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "s-XH_nvE5VzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kvRDD = spark.parallelize([(3,4), (3,1), (5,6), (1,2)])\n",
        "kvRDD.reduceByKey(lambda x, y: x+y).collect()  # here, x=current values, y=next values"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(3, 5), (5, 6), (1, 2)]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639390134408
        },
        "id": "8G40XRCT5VzE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6dc271a-e00c-4d32-b21d-0220986fbf63"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.7 Multiple *key-value* pair transformations\n",
        "Here, we test operators for **joining** multiple key-pair RDDs"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "Jabck3N75VzE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.7.1 ``join()``\n",
        "We can use ``join()`` to combine multiple key-value pair RDD"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "GbqCwGVG5VzE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.7.1.1 example \n",
        "We will **inner join** two RDDs. Here, two elements will join only if they shared same keys.\n",
        "For example {(1,2), (3,4), (3,6)} join {(3,7)} = {(3,(4,7)), (3,(6,7))}"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "JKDLTf7G5VzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kvRDD1 = spark.parallelize([(1,2), (3,4), (3,6)])\n",
        "kvRDD2 = spark.parallelize([(3,7)])\n",
        "kvRDD1.join(kvRDD2).collect()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639390139549
        },
        "id": "K4jqw_zS5VzE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.7.2 ``leftOuterJoin()``\n",
        "For ``leftOuterJoin()``, key must exist in the **left** RDD"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "Hmum9s945VzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kvRDD1 = spark.parallelize([(1,2), (3,4), (3,6)])\n",
        "kvRDD2 = spark.parallelize([(3,7)])\n",
        "result = kvRDD1.leftOuterJoin(kvRDD2) \n",
        "result.collect()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639390144830
        },
        "id": "oLy6jDFT5VzE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise:\n",
        "In example 2.7.2, we saw \"None\" in result variable. How can we replace *None* with \"error\"?"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "seaKKg9q5VzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result.map(lambda x: (x[0], ('error' if x[1][0] is None else x[1][0], 'error' if x[1][1] is None else x[1][1]))).collect()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639390150078
        },
        "id": "NGa8Si0V5VzF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.7.3 ``rightOuterJoin()``\n",
        "For ``rightOuterJoin()``, key must exist in the **right** RDD"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "GqedZmwh5VzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kvRDD1 = spark.parallelize([(1,2), (3,4), (3,6)])\n",
        "kvRDD2 = spark.parallelize([(3,7)])\n",
        "result = kvRDD1.rightOuterJoin(kvRDD2) \n",
        "result.collect()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639390155188
        },
        "id": "7qWjL15S5VzF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.7.4 ``subtractByKey()``\n",
        "For ``subtractByKey()``, a key appears in both RDD will be removed"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "pq50U4MZ5VzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kvRDD1 = spark.parallelize([(1,2), (3,4), (3,6)])\n",
        "kvRDD2 = spark.parallelize([(3,7)])\n",
        "result = kvRDD1.subtractByKey(kvRDD2) \n",
        "result.collect()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639390160411
        },
        "id": "Y6v6eXgw5VzF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.8 Key Value RDD actions\n",
        "Here, we introduce few action opertors, which apply on key-value pair"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "uTSnDN3u5VzF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.8.1 ``first()``\n",
        "Similar to example 2.4.1, we use ``first()`` to get the first element of key and value  "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "diWsMlsj5VzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kvRDD1 = spark.parallelize([(1,2), (3,4), (3,6)])\n",
        "\n",
        "# Get first element\n",
        "print(\"First key-value: %s %s\" % kvRDD1.first())\n",
        "\n",
        "# Get first key\n",
        "print(\"First key: %s\" % kvRDD1.keys().first()) \n",
        "\n",
        "# Get first value\n",
        "print(\"First value: %s\" % kvRDD1.values().first()) \n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639390165669
        },
        "id": "Xv9hLHNr5VzG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.8.2 ``take()``\n",
        "Similar to example 2.4.2, we use ``take()`` to get the first N elements of key and value  "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "VWDFN_LO5VzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kvRDD1 = spark.parallelize([(1,2), (3,4), (3,6), (4,1), (5,6)])\n",
        "\n",
        "# Get first element\n",
        "print(\"First 2 key-value:\", kvRDD1.take(2))\n",
        "\n",
        "# Get first key\n",
        "print(\"First 2 key:\", kvRDD1.keys().take(2)) \n",
        "\n",
        "# Get first value\n",
        "print(\"First2 value:\", kvRDD1.values().take(2)) "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639390170809
        },
        "id": "jKPdPgps5VzG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.8.2 ``countByKey()`` and ``countByValue()``\n",
        "``countByKey()`` and ``countByValue()`` count the frequency of each **key** and **value** (resp.), \n",
        "and return the count as **defaultdict** "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "DukSugjz5VzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kvRDD1 = spark.parallelize([(1,2), (3,4), (3,6), (4,1), (5,6)])\n",
        "\n",
        "# Get frequency of key\n",
        "print(\"Key Frequency:\", kvRDD1.countByKey())\n",
        "\n",
        "# Get frequency of value\n",
        "print(\"Value Frequency:\", kvRDD1.countByValue())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639390175913
        },
        "id": "gA0X41j85VzG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.9 RDD Transformation (laziness)\n",
        "Transformation will **not process** until we call **action**. \n",
        "For example, if we import an unexisting file, no **error** found.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "cHAt2CXR5VzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_test = spark.textFile(\"../data/no_such_file.txt\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639390181085
        },
        "id": "vxO5k6un5VzG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem comes only when we call action (e.g., collect)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "TQB2xVY85VzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_test.collect()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "ykbH0nGr5VzH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.9.1 Advantage of Laziness\n",
        "Lazy evaluation means that if you tell Spark to operate on a set of data, it listens to what you ask it to do, writes down some **shorthand** for it so it doesn’t forget, and then does absolutely nothing. \n",
        "It will continue to do nothing, until you ask it for the final answer.\n",
        "It waits until you’re done giving it operators, and only when you ask it to give you the final answer does it evaluate, and it always looks to limit how much work it has to do.\n",
        "\n",
        "ref: https://stackoverflow.com/questions/38027877/spark-transformation-why-is-it-lazy-and-what-is-the-advantage"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "Ikz2nM_y5VzH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.10 Exercise: Word Count\n",
        "Write a module to read the \"wordCount.txt\" file, split the word by space and do a word count and sort the result descendingly. \n",
        "\n",
        "Hints: You may need to use the following functions:\n",
        "\n",
        "* `textFile()`\n",
        "* `flatMap()`\n",
        "* `reduceByKey()`\n",
        "* `sortBy()`"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "xUNHseeR5VzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your Answer\n",
        "\n",
        "\n",
        "textFile = spark.textFile(data_path + \"wordCount.txt\") \n",
        "data = textFile.flatMap(lambda x: x.split(' ')) \n",
        "\n",
        "counts = data.map(lambda x: \n",
        "\t(x, 1)).reduceByKey(add).sortBy(lambda x: x[1],\n",
        "\t ascending=False).collect()\n",
        "\n",
        "for (word, count) in counts:\n",
        "    print(\"{}: {}\".format(word, count))\n",
        "\n",
        "\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "studying: 2\n",
            "in: 2\n",
            "Computer: 2\n",
            "I: 1\n",
            "am: 1\n",
            "Billy: 1\n",
            "Ada: 1\n",
            "is: 1\n"
          ]
        }
      ],
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639207889286
        },
        "id": "0dIR_I6L5VzH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f549c5a2-49be-41b6-8d4f-ef0e45c616dc"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "colab": {
      "name": "T2_rdd_operator_lab.v2.ipynb",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "sBamBI6t5Vyw",
        "hLOWe0FM5Vyy",
        "khN2qxcz5Vy0",
        "sQu2nOvS5Vy0",
        "mzaGpkM45Vy1",
        "THenZpid5Vy1",
        "a87SxVr75Vy1",
        "dV74cghO5Vy2",
        "Y2LvgXSc5Vy2",
        "WBRT1Vgq5Vy2",
        "9t9qW5wL5Vy3",
        "fXKNAoGB5Vy3",
        "CzpWNud05Vy3",
        "Ha1n0QPP5Vy3",
        "0Cz-ab9y5Vy4",
        "u942vcfv5Vy4",
        "bFb10IPN5Vy5",
        "l_YHqaEP5Vy5",
        "a46rchbV5Vy6",
        "AYtMauXT5Vy6",
        "ACbY7BtG5Vy6",
        "FILi4JyF5Vy6",
        "z9_XezJV5Vy6",
        "YLMzsu3g5Vy7",
        "-HjjJCOM5Vy7",
        "VJziduk15Vy7",
        "gk-evxR55Vy7",
        "1lUksZ6N5Vy8",
        "WHoy6vFm5Vy8",
        "dx5gwEqV5Vy8",
        "KsUSheK-5Vy8",
        "E1vzCc_j5Vy8",
        "tYbo44h85Vy9",
        "uLAlA2lB5Vy9",
        "i8GbhbG25Vy9",
        "OWs8D_fe5Vy-",
        "EiNnEzIH5Vy-",
        "yth_HQ1i5Vy-",
        "ndle9h2X5Vy-",
        "rFL3IpzH5Vy-",
        "Y4bgotPD5Vy-",
        "ZuuiDkKu5Vy-",
        "0jI3dwCn5Vy_",
        "pxUal6xl5Vy_",
        "Zo8yzmOi5Vy_",
        "BSwLEQLK5Vy_",
        "KSuJqqUs5Vy_",
        "8_ndNXpw5VzA",
        "aQXRq3e85VzA",
        "klF2tQIM5VzA",
        "LPle_gCj5VzA",
        "aEtmY3e95VzA",
        "383l-Z1m5VzB",
        "ZO1XiBNT5VzB",
        "vgwfjIIV5VzB",
        "-HuQ6Wn55VzB",
        "2jZwKqYF5VzB",
        "Gc6b6cHa5VzC",
        "TE9NZ8KP5VzC",
        "WvERucjk5VzC",
        "JNiMMCAN5VzD",
        "PM4iiq8I5VzD",
        "TRm1KYQ85VzD",
        "uEjavedV5VzD",
        "s-XH_nvE5VzE",
        "Jabck3N75VzE",
        "GbqCwGVG5VzE",
        "JKDLTf7G5VzE",
        "Hmum9s945VzE",
        "seaKKg9q5VzF",
        "GqedZmwh5VzF",
        "pq50U4MZ5VzF",
        "uTSnDN3u5VzF",
        "diWsMlsj5VzF",
        "VWDFN_LO5VzG",
        "DukSugjz5VzG",
        "cHAt2CXR5VzG",
        "Ikz2nM_y5VzH",
        "xUNHseeR5VzH"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}